{
    "map": {
        "data_dir": "Map/map_1car2region_wgc_5.tsv"
    },
    "trip_demand": {
        "parameter_source": "given",
        "arrival_type": "poisson",
        "parameter_fname": "TripDemand/trip_demand_1car2region_wgc_5.tsv",
        "data": null
    },
    "reward": {
        "reward_fname": "Payoff/payoff_1car2region_wgc.tsv"
    },
    "mdp": {
        "time_horizon": 20,
        "connection_patience": 0,
        "pickup_patience": 0,
        "num_battery_levels": 1,
        "battery_jump": 0.3,
        "force_charging": true,
        "use_charging_curve": false,
        "charging_rates": [
            2
        ],
        "battery_offset": 0,
        "battery_per_step": 0,
        "normalize_by_tripnums": true,
        "total_revenue_benchmark": 10,
        "car_deployment_type": "fixed",
        "region_battery_car_fname": "RegionBatteryCar/region_battery_car_1car2region_wgc.tsv",
        "region_rate_plug_fname": "RegionRatePlug/region_rate_plug_1car2region_wgc.tsv"
    },
    "solver": {
        "type": "ppo"
    },
    "neural": {
        "value_model_name": "discretized_feedforward",
        "value_hidden_dim_lst": [64, 64, 64],
        "value_activation_lst": ["tanh", "relu", "tanh"],
        "value_batch_norm": false,
        "value_lr": 1e-3,
        "value_epoch": 100,
        "value_batch": 100,
        "value_decay": 0.1,
        "value_scheduler_step": 1000,
        "value_solver": "Adam",
        "value_retrain": true,
        "policy_model_name": "discretized_feedforward",
        "policy_hidden_dim_lst": [64, 64, 64],
        "policy_activation_lst": ["tanh", "tanh", "tanh"],
        "policy_batch_norm": false,
        "policy_lr": 1e-3,
        "policy_epoch": 3,
        "policy_batch": 100,
        "policy_decay": 0.1,
        "policy_scheduler_step": 1000,
        "policy_solver": "Adam",
        "policy_retrain": true,
        "descriptor": "PPO",
        "dir": ".",
        "ts_per_network": 1,
        "embedding_dim": 5,
        "num_itr": 20,
        "num_episodes": 100,
        "num_days": 1,
        "normalize_input": false,
        "network_horizon_repeat": 1,
        "useful_days": 2,
        "gamma": 1,
        "eval_days": 1,
        "ckpt_freq": 1000,
        "benchmarking_policy": "uniform",
        "eps": 0.1,
        "eps_eta": 1,
        "eps_sched": 1,
        "policy_syncing_freq": 1,
        "n_cpu": 1,
        "lazy_removal": true,
        "state_reduction": true
    },
    "metric": ["total_payoff"],
    "report": {
        "plot": ["training_loss"],
        "table": [],
        "eval_days": 1,
        "gamma": 1
    }
}
