{
    "map": {
        "data_dir": "Map/map_50car6region300chargers5mins_fullycharged_nyc_combo_fullday.tsv"
    },
    "trip_demand": {
        "parameter_source": "given",
        "arrival_type": "poisson",
        "parameter_fname": "TripDemand/trip_demand_50car6region300chargers5mins_fullycharged_nyc_combo_fullday.tsv",
        "data": null,
        "scaling_factor": 1
    },
    "reward": {
        "reward_fname": "Payoff/payoff_50car6region300chargers5mins_fullycharged_nyc_combo_fullday.tsv"
    },
    "mdp": {
        "time_horizon": 288,
        "connection_patience": 1,
        "pickup_patience": 1,
        "num_battery_levels": 132,
        "battery_jump": 0.3,
        "charging_rates": [
            25
        ],
        "battery_offset": 0,
        "battery_per_step": 1,
        "normalize_by_tripnums": true,
        "battery_cutoff": [45, 90],
        "region_battery_car_fname": "RegionBatteryCar/region_battery_car_50car6region300chargers5mins_fullycharged_nyc_combo_fullday.tsv",
        "region_rate_plug_fname": "RegionRatePlug/region_rate_plug_50car6region300chargers5mins_fullycharged_nyc_combo_fullday.tsv"
    },
    "solver": {
        "type": "ppo"
    },
    "neural": {
        "value_model_name": "discretized_feedforward",
        "value_hidden_dim_lst": [500, 64, 64],
        "value_activation_lst": ["tanh", "relu", "tanh"],
        "value_batch_norm": false,
        "value_lr": 1e-2,
        "value_epoch": 100,
        "value_batch": 100,
        "value_decay": 0.1,
        "value_scheduler_step": 50,
        "value_solver": "Adam",
        "value_retrain": true,
        "policy_model_name": "discretized_feedforward",
        "policy_hidden_dim_lst": [500, 64, 64],
        "policy_activation_lst": ["tanh", "tanh", "tanh"],
        "policy_batch_norm": false,
        "policy_lr": 5e-4,
        "policy_epoch": 3,
        "policy_batch": 100,
        "policy_decay": 0.1,
        "policy_scheduler_step": 1000,
        "policy_solver": "Adam",
        "policy_retrain": true,
        "descriptor": "PPO",
        "dir": ".",
        "num_itr": 80,
        "num_episodes": 500,
        "num_days": 1,
        "network_horizon_repeat": 1,
        "useful_days": 2,
        "gamma": 0.97,
        "eval_days": 1,
        "ckpt_freq": 10,
        "benchmarking_policy": "uniform",
        "eps": 0.1,
        "eps_sched": 1,
        "eps_eta": 0.97,
        "policy_syncing_freq": 1,
        "n_cpu": 8,
        "lazy_removal": true,
        "state_reduction": true
    },
    "metric": ["total_payoff"],
    "report": {
        "plot": ["training_loss"],
        "table": [],
        "eval_days": 1,
        "gamma": 0.97
    }
}
